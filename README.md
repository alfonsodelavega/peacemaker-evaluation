# Peacemaker performance evaluation

This instructions explain how to run the performance evaluation used to compare [Peacemaker](https://github.com/epsilonlabs/peacemaker) with model-based model merge approaches. Scripts are provided to generate models and for running the benchmarks.

When not mentioned otherwise, instructions are to be run from the `org.eclipse.epsilon.peacemaker.benchmarks` directory. These instructions have been tested in Mac OS Catalina and Ubuntu 20.04.

## Compiling the benchmarks

Maven is required for this process. Some of the dependencies need to be installed locally, as explained below.

### Locally installing Peacemaker

1. Clone the [Peacemaker source repository](https://github.com/epsilonlabs/peacemaker) into your system
2. From the `org.eclipse.epsilon.peacemaker` folder, run `mvn clean install` in a terminal. This will install a jar with the core Peacemaker code in your local Maven repository.

### Locally installing EMF Compare

Unfortunately, EMF Compare (i.e. one of the compared approaches) does not provide jars in the standard Maven public repositories. To locally install an individual jar of EMF Compare, run `mvn package` from the `org.eclipse.epsilon.peacemaker.benchmarks` repository. This manually installs the EMF Compare jar present under the `lib` folder in your local Maven repository.

### Generating `target/benchmarks.jar`

To generate the `benchmarks.jar` that allows invoking the [JMH](https://github.com/openjdk/jmh) benchmarks, run `mvn clean verify` from the `org.eclipse.epsilon.peacemaker.benchmarks` folder.

## Generating the Benchmark models

The models required for the performance comparison (~6 GiB) can be generated by running the `generate_models.sh` script.

## Running the benchmarks

To run the benchmarks, execute the `run_benchmarks.sh` script. This execution can take a few hours. You can also run a faster `test_benchmarks.sh` execution first to make sure the evaluation finishes properly first.

## Plotting the results

`org.eclipse.epsilon.peacemaker.benchmarks/plots/scenarios.py` contains the Python code used to generate plot figures of the results and calculates relative improvements of using one tool or the other.